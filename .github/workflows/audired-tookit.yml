name: Audi RED Toolkit

on:
  workflow_call:
    inputs:
      source_file:
        description: "Source file from the origin directory"
        required: true
        type: string
      destination_repo:
        description: "Destination repository"
        type: string
        required: false
        default: "RED-Internal-Development/audi-red-documentation"
      destination_folder:
        description: "Directory to push the file to"
        type: string
        required: false
      user_email:
        description: "Email for the git commit"
        type: string
        required: true
      user_name:
        description: "GitHub username for the commit"
        type: string
        required: true
      user_actor:
        description: "GitHub username that trigged the pipeline"
        type: string
        required: true
      destination_branch:
        description: "branch to push file to, defaults to main"
        type: string
        required: false
      destination_branch_create:
        description: "Destination branch to create for this commit"
        type: string
        required: false
      commit_message:
        description: "A custom message for the commit"
        type: string
        required: false
      rename:
        description: "Rename the destination file"
        type: string
        required: false
      use_rsync:
        description: "Copy files/directories using rsync instead of cp. Experimental feature, please know your use case"
        type: string
        required: false
      git_server:
        description: "Git server host, default github.com"
        type: string
        required: false
        default: github.com
      msiParentPageIds:
        description: "By default we deployment all feature apps to a documentation sync parent id in msi. If you prefer to deploy elsewhere in this space, list ids"
        default: ""
        type: string
        required: false
      enable_doc_sync:
        description: "Enable doc sync step, DOC_SYNC_KEY is required in secrets"
        type: boolean
        required: true
      enable_scanoss:
        description: "[Deprecated] Ignored. Kept for backward compatibility."
        type: boolean
        required: false
        default: false
      enable_vwgoa_prod_support_deployment:
        description: "Enable app deployment to VWGOA Production Support Space"
        type: boolean
        required: false
        default: false
      project_type:
        description: "Type of project: 'frontend-app', 'backend-service', or 'auto-detect'. Auto-detected if not specified."
        type: string
        required: false
        default: "auto-detect"
    secrets:
      DOC_SYNC_KEY:
        required: false
        description: "Team key used to copy files to audired"
      SCANOSS_KEY:
        required: false
        description: "[Deprecated] Ignored. Keeping for backward compatibility."
env:
  NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  detect_project:
    runs-on: ubuntu-latest
    outputs:
      project_type: ${{ steps.detect.outputs.project_type }}
      has_jest: ${{ steps.detect.outputs.has_jest }}
      has_cypress: ${{ steps.detect.outputs.has_cypress }}
      docs_base_path: ${{ steps.detect.outputs.docs_base_path }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Detect project type
        id: detect
        shell: bash
        run: |
          INPUT_TYPE="${{ inputs.project_type }}"
          
          # If explicitly set, use it
          if [[ "$INPUT_TYPE" != "auto-detect" ]]; then
            echo "project_type=$INPUT_TYPE" >> $GITHUB_OUTPUT
            
            if [[ "$INPUT_TYPE" == "backend-service" ]]; then
              echo "docs_base_path=backend_services" >> $GITHUB_OUTPUT
            else
              echo "docs_base_path=feature_apps" >> $GITHUB_OUTPUT
            fi
          else
            # Auto-detect based on package.json and file structure
            HAS_CYPRESS=false
            HAS_JEST=false
            
            if [ -f "package.json" ]; then
              # Check for Cypress
              if jq -e '.scripts | to_entries[] | select(.value | contains("cypress"))' package.json > /dev/null 2>&1; then
                HAS_CYPRESS=true
              fi
              
              if [ -d "cypress" ]; then
                HAS_CYPRESS=true
              fi
              
              # Check for Jest
              if jq -e '.scripts | to_entries[] | select(.key | test("test|jest"))' package.json > /dev/null 2>&1; then
                HAS_JEST=true
              fi
              
              if [ -f "jest.config.js" ] || [ -f "jest.config.ts" ] || jq -e '.jest' package.json > /dev/null 2>&1; then
                HAS_JEST=true
              fi
            fi
            
            # Determine project type
            if [[ "$HAS_CYPRESS" == "true" ]]; then
              DETECTED_TYPE="frontend-app"
              DOCS_PATH="feature_apps"
            elif [[ "$HAS_JEST" == "true" ]]; then
              DETECTED_TYPE="backend-service"
              DOCS_PATH="backend_services"
            else
              # Default to frontend-app for backward compatibility
              DETECTED_TYPE="frontend-app"
              DOCS_PATH="feature_apps"
            fi
            
            echo "project_type=$DETECTED_TYPE" >> $GITHUB_OUTPUT
            echo "docs_base_path=$DOCS_PATH" >> $GITHUB_OUTPUT
          fi
          
          # Always output test framework availability
          echo "has_cypress=$HAS_CYPRESS" >> $GITHUB_OUTPUT
          echo "has_jest=$HAS_JEST" >> $GITHUB_OUTPUT
          
          echo "âœ… Detected project type: ${DETECTED_TYPE:-$INPUT_TYPE}"
          echo "âœ… Cypress available: $HAS_CYPRESS"
          echo "âœ… Jest available: $HAS_JEST"
          echo "âœ… Docs base path: ${DOCS_PATH:-$([ \"$INPUT_TYPE\" == \"backend-service\" ] && echo backend_services || echo feature_apps)}"

  doc_sync:
    runs-on: ubuntu-22.04
    if: ${{ always() && inputs.enable_doc_sync }}
    needs: [detect_project, data_sync]
    steps:
      - name: Check if branch exists
        id: check_branch
        shell: bash
        run: |
          BRANCH_NAME="${{ inputs.destination_branch }}"
          DESTINATION_REPO="${{ inputs.destination_repo }}"
          API_URL="https://api.github.com/repos/${DESTINATION_REPO}/branches/${BRANCH_NAME}"

          # Query the GitHub API
          response=$(curl -s -o /dev/null -w "%{http_code}" -H "Authorization: Bearer ${{ secrets.DOC_SYNC_KEY }}" "$API_URL")

          if [ "$response" -eq 200 ]; then
              echo "Branch '${BRANCH_NAME}' exists."
              echo "branch_exists=true" >> $GITHUB_OUTPUT
          else
              echo "Branch '${BRANCH_NAME}' does not exist."
              echo "branch_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Checkout
        uses: actions/checkout@v4

      - name: Eslint check for Docusaurus build compatibility
        run: |
          cd ${{ inputs.source_file }}
          npx docusaurus-mdx-checker

      - name: Install mermaid CLI for parsing
        run: npm install -g @mermaid-js/mermaid-cli

      - name: Validate mermaid.js code can be parsed
        run: |
          mkdir -p diagrams
          folder=${{ inputs.source_file }}
          index=1
          find "$folder" -type f \( -name "*.md" -o -name "*.mdx" \) -print0 | while IFS= read -r -d '' file; do
            echo "Processing markdown file: $file"
            
            in_code_block=false
            mermaid_code=""
            temp_file=$(mktemp)
            TMPDIR=$(mktemp -d)

            while IFS= read -r line; do
              if [[ "$line" == '```mermaid' ]]; then
                in_code_block=true
                mermaid_code=""
                continue
              elif [[ "$line" == '```' && "$in_code_block" == true ]]; then
                echo "Found Mermaid diagram:"
                echo "$mermaid_code"
                
                # Write the Mermaid code to a temporary .mmd file and generate the SVG
                diagram_name="diagram_${index}.mmd"
                echo "$mermaid_code" > "diagrams/${diagram_name}"

                if ! mmdc -i "diagrams/${diagram_name}" -o "$TMPDIR/output_${index}.svg" 2> "$TMPDIR/mmdc_error.log"; then
                  echo "âŒ Mermaid validation failed in file:"
                  cat "$TMPDIR/mmdc_error.log"
                  exit 1
                fi
                
                in_code_block=false
                index=$((index + 1))
              elif [[ "$in_code_block" == true ]]; then
                mermaid_code="$mermaid_code$line"$'\n'
              else
                echo "$line" >> "$temp_file"
              fi
            done < "$file"
          done
          echo "âœ… All Mermaid blocks are valid!"

      - name: Create images from structurizr dsl files
        uses: RED-Internal-Development/audired_structurizr_action@main

      - name: Check if referenced images exist
        run: |
          find docs/ -type f \( -name "*.md" -o -name "*.mdx" \) -print0 | while IFS= read -r -d '' md_file; do
                echo "Checking file: $md_file"
                
                grep -oP '!\[.*?\]\(\K(.*?)(?=\))' "$md_file" | while read -r image; do
                    image=$(echo "$image" | sed 's/[?#].*$//')
                    if [[ "$image" =~ ^https?:// ]]; then
                        echo "Skipping external image: $image"
                        continue
                    elif [[ "$image" =~ ^/ ]]; then
                        IMAGE_PATH="$GITHUB_WORKSPACE$image"
                    else
                        IMAGE_PATH="$(dirname "$md_file")/$image"
                    fi

                    IMAGE_PATH=$(realpath "$IMAGE_PATH")

                    if [[ ! -f "$IMAGE_PATH" ]]; then
                        echo "Image '$image' referenced in '$md_file' does not exist at '$IMAGE_PATH'."
                        exit 1
                    fi
                done
            done

            echo "All images are properly referenced and exist!"

      - name: Download collection report artifact
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: data-report
          path: data-report

      - name: Verify artifact and create fallback if needed
        run: |
          if [ -f "data-report/report.json" ]; then
            echo "âœ… Artifact downloaded successfully"
            ls -la data-report/
          else
            echo "âš ï¸ Artifact not found - creating empty report for backend services"
            mkdir -p data-report
            echo '{}' > data-report/report.json
          fi

      - name: Create or update project metrics with report data
        run: |
          FILE_PATH="${{ inputs.source_file }}/project_metrics.mdx"
          report_file="data-report/report.json"
          report_file_data=$(cat $report_file)

          if [ -z "$report_file_data" ]; then
              echo "No report results to include, skipping"
          else
              repo_name=$(echo "$report_file_data" | jq -r 'to_entries | .[0].key')
              echo "REPO: $repo_name"
              lighthouse_score=$(jq ".[\"$repo_name\"].lighthouse_score" "$report_file")
              echo "lighthouse score: $lighthouse_score"
              unit_test_coverage_data=$(jq ".[\"$repo_name\"].unit_test_coverage" "$report_file")
              echo "unit test coverage: $unit_test_coverage_data"
              e2e_test_coverage=$(jq ".[\"$repo_name\"].e2e_test_coverage" "$report_file")
              echo "e2e test coverage: $e2e_test_coverage"

              NEW_CONTENT="# CI Report Summary\n"
              if [ -n "$lighthouse_score" ]  && [ "$lighthouse_score" != "null" ]; then
                NEW_CONTENT+="- **Lighthouse Score**: $lighthouse_score / 1\n"
              fi

              if [ -n "$e2e_test_coverage" ] && [ "$e2e_test_coverage" != "null" ]; then
                rounded_e2e_test_coverage=$(printf "%.0f" $e2e_test_coverage)
                NEW_CONTENT+="- **E2E Test Coverage**: $rounded_e2e_test_coverage%\n"
              fi

              if [ -n "$unit_test_coverage_data" ] && [ "$unit_test_coverage_data" != "null" ]; then
                echo "checking unit_test_coverage_data"
                check_value() {
                  local value=$1
                  value=$(echo "$value" | awk '{print $1+0}')
                  
                  if (( $(echo "$value > 80" | bc -l) )); then
                    echo ":white_check_mark:"
                  elif (( $(echo "$value > 50" | bc -l) )); then
                    echo ":warning:"
                  else
                    echo ":x:"
                  fi
                }
                
                unit_statements=$(jq ".[\"$repo_name\"].unit_test_coverage_data.statement_coverage" "$report_file")
                unit_functions=$(jq ".[\"$repo_name\"].unit_test_coverage_data.function_coverage" "$report_file")
                unit_branches=$(jq ".[\"$repo_name\"].unit_test_coverage_data.branch_coverage" "$report_file")
                unit_lines=$(jq ".[\"$repo_name\"].unit_test_coverage_data.line_coverage" "$report_file")
                unit_average=$(jq ".[\"$repo_name\"].unit_test_coverage_data.average_coverage" "$report_file")
                echo "unit_average: $unit_average"
                rounded_unit_test_coverage=$(printf "%.0f" $unit_average)
                echo "rounded_unit_test_coverage: $rounded_unit_test_coverage"
                NEW_CONTENT+=$(
                  echo "## Unit Test Coverage" $'\n' \
                      "| Category | Coverage | Rating |" $'\n' \
                      "|-------------|------------|----------------|" $'\n' \
                      "| Statements | "$unit_statements%" | $(check_value $unit_statements) |" $'\n' \
                      "| Functions  | "$unit_functions%" | $(check_value $unit_functions) |" $'\n' \
                      "| Branches   | "$unit_branches%" | $(check_value $unit_branches) |" $'\n' \
                      "| Lines      | "$unit_lines%" | $(check_value $unit_lines) |" $'\n' \
                      "| Overall    | "$rounded_unit_test_coverage%" | $(check_value $rounded_unit_test_coverage) |" $'\n'
                )

                RECOMMENDATIONS_CONTENT=""
                if (( $(echo "$unit_branches < 60" | bc -l) )); then
                  RECOMMENDATIONS_CONTENT+="- Test more branches (if/else, error handling)"
                fi

                if (( $(echo "$unit_functions < 70" | bc -l) )); then
                  RECOMMENDATIONS_CONTENT+="- Write additional unit tests for functions."
                fi

                if (( $(echo "$unit_lines < 70" | bc -l) )); then
                  RECOMMENDATIONS_CONTENT+="- Better cover the entire code (e.g. rare code paths)"
                fi

                if [[ -n "RECOMMENDATIONS_CONTENT" ]]; then
                  NEW_CONTENT+=$(
                    echo $'\n' "### :pushpin: Recommendations:" $'\n' \
                        "$RECOMMENDATIONS_CONTENT" $'\n'
                  )
                fi
              elif [ -n "$unit_test_coverage" ] && [ "$unit_test_coverage" != "null" ]; then
                rounded_unit_test_coverage=$(printf "%.0f" $unit_test_coverage)
                NEW_CONTENT+="- **Unit Test Coverage**: $rounded_unit_test_coverage%\n"
              fi

              # Check if the file exists
              if [ -f "$FILE_PATH" ]; then
                echo "File exists. Appending content to the top of the file: $FILE_PATH"
                echo -e "$NEW_CONTENT\n$(cat $FILE_PATH)" > $FILE_PATH
              else
                echo "File does not exist. Creating a new file: $FILE_PATH"
                echo -e "$NEW_CONTENT" > $FILE_PATH
              fi
          fi

      - name: Push documentation to Audi RED Portal for syndication
        uses: RED-Internal-Development/audred_docsync_action@main
        env:
          API_TOKEN_GITHUB: ${{ secrets.DOC_SYNC_KEY }}
        with:
          source_file: ${{ inputs.source_file }}
          destination_repo: ${{ inputs.destination_repo }}
          destination_folder: ${{ needs.data_sync.outputs.docs_destination_team_folder }}
          destination_branch: ${{ inputs.destination_branch }}
          rename: ${{ needs.data_sync.outputs.app_name }}
          user_email: ${{ inputs.user_email }}
          user_name: ${{ inputs.user_name }}
          user_actor: ${{ github.actor }}
          use_rsync: true
          destination_branch_exists: ${{ steps.check_branch.outputs.branch_exists }}

  data_sync:
    runs-on: ubuntu-latest
    if: always()
    needs: [detect_project, cypress_sync, jest_sync]
    outputs:
      docs_destination_team_folder: ${{ steps.create-docs-folders.outputs.docs_destination_team_folder }}
      docs_destination_app_folder: ${{ steps.create-docs-folders.outputs.docs_destination_app_folder }}
      app_name: ${{ steps.create-docs-folders.outputs.app_name }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Collection of metadata from repository
        id: metadata_collection
        uses: RED-Internal-Development/audi-red-app-metadata-kit@staging
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          repository: ${{ github.repository }}
          workflow_run_id: ${{ github.event.workflow_run.id }}

      - name: Checkout AudiRed Doc Sync repository
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.destination_repo }}
          token: ${{ secrets.DOC_SYNC_KEY }}
          ref: doc-sync-queue

      - name: Download metadata artifact
        uses: actions/download-artifact@v4
        with:
          name: metadata-report
          path: metadata-report

      - name: Download cypress report artifact
        if: github.event_name == 'schedule' && needs.cypress_sync.outputs.cypress_run == 'true'
        uses: actions/download-artifact@v4
        with:
          name: audired-cypress-report
          path: audired-cypress-report

      - name: Download jest report artifact
        if: |
          (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch') &&
          needs.jest_sync.outputs.jest_run == 'true'
        uses: actions/download-artifact@v4
        with:
          name: audired-jest-report
          path: audired-jest-report

      - name: Download collection report artifact
        if: github.event.workflow_run.id != ''
        uses: actions/download-artifact@v4
        with:
          name: audired-collection-report
          path: collection-report
          github-token: ${{ github.token }}
          repository: ${{ github.repository }}
          run-id: ${{ github.event.workflow_run.id }}

      - name: Dynamically create docs destination folder variables
        id: create-docs-folders
        run: |
          app_name=$(basename ${{ inputs.source_file }})
          docs_branch="${{ inputs.destination_branch }}"
          project_type="${{ needs.detect_project.outputs.project_type }}"
          
          # Fix: If app_name is "docs" (backend services with docs at root), use repository name instead
          if [[ "$app_name" == "docs" ]]; then
            # Extract repository name from github.repository (owner/repo-name)
            app_name=$(basename "${{ github.repository }}")
            echo "ðŸ“ Detected docs at root - using repository name: $app_name"
          fi
          
          # Determine paths based on project type
          if [[ "$project_type" == "backend-service" ]]; then
            # Backend services: no team folder, goes directly under backend_services/
            docs_destination_team_folder="docs/backend_services"
            docs_destination_app_folder="$docs_destination_team_folder/$app_name"
          else
            # Frontend apps: include team folder (destination_branch)
            docs_base="${{ needs.detect_project.outputs.docs_base_path || 'feature_apps' }}"
            docs_destination_team_folder="docs/$docs_base/${{ inputs.destination_branch }}"
            docs_destination_app_folder="$docs_destination_team_folder/$app_name"
          fi

          echo "docs_destination_team_folder=$docs_destination_team_folder" >> $GITHUB_OUTPUT
          echo "docs_destination_app_folder=$docs_destination_app_folder" >> $GITHUB_OUTPUT
          echo "app_name=$app_name" >> $GITHUB_OUTPUT
          echo "docs_branch=$docs_branch" >> $GITHUB_ENV
          echo "docs_destination_team_folder=$docs_destination_team_folder" >> $GITHUB_ENV
          echo "docs_destination_app_folder=$docs_destination_app_folder" >> $GITHUB_ENV
          
          echo "ðŸ“ Project type: $project_type"
          echo "ðŸ“ Docs destination folder: $docs_destination_team_folder"
          echo "ðŸ“ App/Service folder: $docs_destination_app_folder"

      - name: Update data/report.json with additional metadata
        run: |
          data_folder="data"
          per_repo_folder="$data_folder/per-repo"
          collection_report_file_path="collection-report/report.json"
          metadata_file_path="metadata-report/metadata-report.json"
          cypress_data_file_path="audired-cypress-report/audired-cypress-report.json"
          output_file_path="combined-data.json"

          metadata_artifact_json=$(cat "$metadata_file_path")
          repo_name=$(echo "$metadata_artifact_json" | jq -r 'to_entries | .[0].key')

          if [[ -n "${{ github.event.workflow_run.id }}" ]]; then
            jq --arg repo_name "$repo_name" \
              '.[ $repo_name ] |= (. + (input | .[ $repo_name ]))' \
              "$collection_report_file_path" "$metadata_file_path" > "$output_file_path"

            report_artifact_json=$(cat $output_file_path)
          else
            mkdir -p "collection-report"
            echo $metadata_artifact_json > $collection_report_file_path
            report_artifact_json=$(cat $collection_report_file_path)
          fi

          if [[ -f "$cypress_data_file_path" ]]; then
            echo "$report_artifact_json" > "$collection_report_file_path"
            jq --arg repo_name "$repo_name" \
              '.[ $repo_name ] |= (. + (input | .[ $repo_name ]))' \
              "$collection_report_file_path" "$cypress_data_file_path" > "$output_file_path"

            report_artifact_json=$(cat "$output_file_path")
          else
            echo "No cypress data artifact found."
          fi

          # Merge Jest data from Pattern A (scheduled runs)
          jest_data_file_path="audired-jest-report/audired-jest-report.json"
          if [[ -f "$jest_data_file_path" ]]; then
            echo "Merging Jest coverage data from scheduled run..."
            echo "$report_artifact_json" > "$collection_report_file_path"
            
            jq --arg repo_name "$repo_name" \
              '.[ $repo_name ] |= (. + (input | .[ $repo_name ]))' \
              "$collection_report_file_path" "$jest_data_file_path" > "$output_file_path"
            
            report_artifact_json=$(cat "$output_file_path")
            echo "âœ… Jest data merged successfully"
          else
            echo "No Jest data from scheduled run to merge"
          fi

          echo "report_artifact_json: $report_artifact_json"

          # prod_support_enabled variable

          prod_support_enabled=false
          if [ "${{ inputs.enable_vwgoa_prod_support_deployment }}" == "true" ]; then
              prod_support_enabled=true
          fi

          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          updated_report_artifact_json=$(echo "$report_artifact_json" | \
            jq --arg repo_name "$repo_name" \
            --argjson prod_support "$prod_support_enabled" \
            --arg docs_destination_team_folder "$docs_destination_team_folder" \
            --arg docs_destination_app_folder "$docs_destination_app_folder" \
            --arg docs_branch "$docs_branch" \
            --arg timestamp "$timestamp" \
            '.[$repo_name] |= . + {prod_support_enabled: $prod_support, docs: {docs_destination_team_folder: $docs_destination_team_folder, docs_destination_app_folder: $docs_destination_app_folder, docs_branch: $docs_branch}, timestamp: $timestamp}')
          echo "$updated_report_artifact_json" > collection-report/report.json
          report_artifact_json=$(cat collection-report/report.json)

          # Push artifact report to existing report.json
          mkdir -p "$data_folder"

          # Check if report.json exists inside the $data_folder folder; if not, create it
          if [ ! -f "$data_folder"/report.json ]; then
              echo "{}" > "$data_folder"/report.json
          fi

          jq --argjson new_report "$report_artifact_json" \
            '. * $new_report' "$data_folder"/report.json > tmp_report.json

          mv tmp_report.json "$data_folder"/report.json

          mkdir -p "$per_repo_folder"
          safe_repo_name=$(echo "$repo_name" | tr '/@ ' '___')
          per_repo_file="$per_repo_folder/$safe_repo_name.json"
          repo_snapshot=$(echo "$report_artifact_json" | jq --arg repo_name "$repo_name" '.[$repo_name]')

          if [ -z "$repo_snapshot" ] || [ "$repo_snapshot" = "null" ]; then
            echo "{}" > "$per_repo_file"
          else
            echo "$repo_snapshot" > "$per_repo_file"
          fi

      - name: Upload final report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-report
          path: collection-report/report.json
          retention-days: 1
          if-no-files-found: warn

      - name: Create Repo Configuration File for MSI/Cloud Deployments
        run: |
          app_name=$(basename ${{ inputs.source_file }})
          project_type="${{ needs.detect_project.outputs.project_type }}"
          
          # Determine team folder name based on project type
          if [[ "$project_type" == "backend-service" ]]; then
            # Backend services: all under "Backend Services" parent
            team_folder_name="Backend Services"
          else
            # Frontend apps: use destination_branch as team name
            team_folder_name="${{ inputs.destination_branch }}"
          fi
          
          msi_parent_page_ids="${{ inputs.msiParentPageIds }}"
          config_file="deployment/repo_configs/$app_name.json"

          mkdir -p "$(dirname "$config_file")"
          
          # NOTE: Config files are ONLY created for apps with special deployment needs:
          # 1. Custom MSI parent page IDs (deploy to additional Confluence pages)
          # 2. VWGOA Production Support deployment enabled
          # 
          # For STANDARD deployments (most apps), config files are DELETED because:
          # - MSI deployment reads from the folder structure (docs/feature_apps/{TEAM}/{APP})
          # - No additional configuration is needed
          # - This keeps the repo clean and reduces maintenance

          if [[ -z "$msi_parent_page_ids" && "${{ inputs.enable_vwgoa_prod_support_deployment }}" == "false" ]]; then
            if [[ -f "$config_file" ]]; then
              rm "$config_file"
            fi
          else
            echo '{}' > "$config_file"
            msi_jq_string=""
            if [[ -n "$msi_parent_page_ids" ]]; then
              msi_array=$(echo "$msi_parent_page_ids" | tr ',' '\n' | jq -R . | jq -s .)
              msi_jq_string=".msi_parent_page_ids = $msi_array | "
            fi

            if [ "${{ inputs.enable_vwgoa_prod_support_deployment }}" == "true" ]; then
              vwgoa_enabled=true
              vwgoa_jq_string=".vwgoa_enabled = $vwgoa_enabled | "
            fi
            
            jq "$msi_jq_string$vwgoa_jq_string.team_folder = \"$team_folder_name\" | .app_name = \"$app_name\"" "$config_file" > "$config_file.tmp" && mv "$config_file.tmp" "$config_file"
            echo "config file:"
            cat "$config_file"
          fi

      - name: Commit and push changes
        run: |
          git config --global user.email ${{ inputs.user_email }}
          git config --global user.name ${{ inputs.user_name }}
          git add data/report.json
          if [ -d data/per-repo ]; then
              git add data/per-repo/
          fi
          git add deployment/
          if ! git diff-index --quiet HEAD; then
              echo "commiting changes"
              git commit -m "Update report.json and deployment config from ${{ inputs.destination_branch }}"
              git push
          else
              echo "No changes to push"
          fi

  jest_sync:
    runs-on: ubuntu-latest
    needs: detect_project
    if: |
      needs.detect_project.outputs.has_jest == 'true' &&
      (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')
    outputs:
      jest_run: ${{ steps.check_script.outputs.exists }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Check if Jest test script exists
        id: check_script
        shell: bash
        run: |
          if jq -e '.scripts["test"] or .scripts["test:unit"] or .scripts["test:coverage"]' package.json > /dev/null; then
            echo "Jest test script found"
            echo "exists=true" >> "$GITHUB_OUTPUT"
          else
            echo "No Jest test script found"
            echo "exists=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Setup Node.js
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/setup-node@v4
        with:
          node-version-file: ".nvmrc"
          cache: npm
          registry-url: https://npm.pkg.github.com

      - name: Install dependencies
        if: steps.check_script.outputs.exists == 'true'
        shell: bash
        run: npm ci
        env:
          NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Run Jest with Coverage
        if: steps.check_script.outputs.exists == 'true'
        shell: bash
        run: |
          # Try different script patterns
          if jq -e '.scripts["test:coverage"]' package.json > /dev/null; then
            npm run test:coverage
          elif jq -e '.scripts["test:unit"]' package.json > /dev/null; then
            npm run test:unit -- --coverage
          else
            npm test -- --coverage
          fi

      - name: Collect Jest Coverage using Collection Kit
        if: steps.check_script.outputs.exists == 'true'
        uses: ./Actions/audi-red-collection-kit_action.yml
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          jest_coverage_file_path: coverage/coverage-summary.json
          lighthouse_coverage_file_path: ""
      
      - name: Rename report for consistency
        if: steps.check_script.outputs.exists == 'true'
        shell: bash
        run: |
          if [ -f "report.json" ]; then
            mv report.json audired-jest-report.json
            echo "âœ… Jest coverage report created via collection-kit"
            cat audired-jest-report.json
          fi

      - name: Upload Jest Coverage Report
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: audired-jest-report
          path: audired-jest-report.json
          retention-days: 30

  cypress_sync:
    runs-on: ubuntu-latest
    needs: detect_project
    if: |
      needs.detect_project.outputs.has_cypress == 'true' &&
      (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')
    outputs:
      cypress_run: ${{ steps.check_script.outputs.exists }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Check if test:e2e-run-dev script exists
        id: check_script
        run: |
          if jq -e '.scripts["test:e2e-run-dev"]' package.json > /dev/null; then
            echo "Script exists. Starting cypress install, run, and coverage collection"
            echo "exists=true" >> "$GITHUB_OUTPUT"
          else
            echo "Script does not exist. Skipping cypress collection"
            echo "exists=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Load Cypress cache
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/Cypress
          key: ${{ runner.os }}-cypress-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-cypress

      - name: Setup node
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/setup-node@v4
        with:
          node-version-file: ".nvmrc"
          cache: npm
          registry-url: https://npm.pkg.github.com

      - name: Install dependencies
        if: steps.check_script.outputs.exists == 'true'
        run: |
          npm ci

      - name: Cypress Coverage Run
        if: steps.check_script.outputs.exists == 'true'
        run: |
          echo "Runs only on a cron job"
          npm run test:e2e-run-dev

      - name: E2E (Cypress) Coverage Collection
        if: steps.check_script.outputs.exists == 'true'
        run: |
          if [ -d ".nyc_output" ]; then
            COVERAGE_OUTPUT=$(npx nyc report --reporter=text-summary)

            echo "Coverage Output: $COVERAGE_OUTPUT"

            extract_raw() {
              echo "$COVERAGE_OUTPUT" | grep -oPm1 "^$1\s*:\s*\K([0-9.]+|Unknown)(?=%)"
            }

            # Pull the *raw* value (may be 'Unknown')
            RAW_STATEMENTS=$(extract_raw Statements)
            RAW_BRANCHES=$(extract_raw Branches)
            RAW_FUNCTIONS=$(extract_raw Functions)
            RAW_LINES=$(extract_raw Lines)

            # Check if all are Unknown first
            if [[ $RAW_STATEMENTS == Unknown && \
                $RAW_BRANCHES   == Unknown && \
                $RAW_FUNCTIONS  == Unknown && \
                $RAW_LINES      == Unknown ]]; then
              echo "::error::Coverage summary returned 'Unknown' for all metrics â€“ cannot proceed."
              exit 1
            fi

            # Warn about individual Unknown metrics
            UNKNOWN_COUNT=0
            [[ $RAW_STATEMENTS == Unknown ]] && { echo "::warning::Statements coverage is unavailable"; ((UNKNOWN_COUNT++)); }
            [[ $RAW_BRANCHES == Unknown ]] && { echo "::warning::Branches coverage is unavailable"; ((UNKNOWN_COUNT++)); }
            [[ $RAW_FUNCTIONS == Unknown ]] && { echo "::warning::Functions coverage is unavailable"; ((UNKNOWN_COUNT++)); }
            [[ $RAW_LINES == Unknown ]] && { echo "::warning::Lines coverage is unavailable"; ((UNKNOWN_COUNT++)); }

            # Convert to numbers (Unknown -> 0)
            to_num() { [[ $1 == Unknown ]] && echo 0 || echo "$1"; }

            STATEMENTS=$(to_num "$RAW_STATEMENTS")
            BRANCHES=$(to_num "$RAW_BRANCHES")
            FUNCTIONS=$(to_num "$RAW_FUNCTIONS")
            LINES=$(to_num "$RAW_LINES")

            echo "Line coverage:      ${LINES}%   (raw: ${RAW_LINES})"
            echo "Statement coverage: ${STATEMENTS}% (raw: ${RAW_STATEMENTS})"
            echo "Function coverage:  ${FUNCTIONS}% (raw: ${RAW_FUNCTIONS})"
            echo "Branch coverage:    ${BRANCHES}%  (raw: ${RAW_BRANCHES})"

            # Calculate average but note if partial
            if [[ $UNKNOWN_COUNT -gt 0 ]]; then
              AVERAGE_COVERAGE=$(echo "($LINES + $STATEMENTS + $FUNCTIONS + $BRANCHES) / 4" | bc -l)
              echo "Average coverage: $AVERAGE_COVERAGE% (Note: $UNKNOWN_COUNT metric(s) unavailable, counted as 0%)"
            else
              AVERAGE_COVERAGE=$(echo "($LINES + $STATEMENTS + $FUNCTIONS + $BRANCHES) / 4" | bc -l)
              echo "Average coverage: $AVERAGE_COVERAGE%"
            fi
          else
            echo ".nyc_output folder does not exist for coverage reporting"
            AVERAGE_COVERAGE=null
          fi

          echo "E2E_TEST_COVERAGE=$AVERAGE_COVERAGE" >> $GITHUB_ENV
          echo "E2E_TEST_COVERAGE_STATEMENTS=$STATEMENTS" >> $GITHUB_ENV
          echo "E2E_TEST_COVERAGE_BRANCHES=$BRANCHES" >> $GITHUB_ENV
          echo "E2E_TEST_COVERAGE_FUNCTIONS=$FUNCTIONS" >> $GITHUB_ENV
          echo "E2E_TEST_COVERAGE_LINES=$LINES" >> $GITHUB_ENV

      - name: Extract repo_name from package.json
        if: steps.check_script.outputs.exists == 'true'
        run: |
          repo_name=$(jq -r '.name' package.json)
          echo "repo_name=$repo_name" >> $GITHUB_ENV

      - name: Create audired-cypress-report.json
        if: steps.check_script.outputs.exists == 'true'
        run: |
          echo "{}" > audired-cypress-report.json
          report_json=$(cat audired-cypress-report.json)

          updated_report=$(echo "$report_json" | jq --arg repo_name "$repo_name" \
              --argjson e2e_test_coverage_statements "$E2E_TEST_COVERAGE_STATEMENTS" \
              --argjson e2e_test_coverage_branches "$E2E_TEST_COVERAGE_BRANCHES" \
              --argjson e2e_test_coverage_functions "$E2E_TEST_COVERAGE_FUNCTIONS" \
              --argjson e2e_test_coverage_lines "$E2E_TEST_COVERAGE_LINES" \
              --argjson e2e_test_coverage "$E2E_TEST_COVERAGE" \
              '. + {($repo_name): {"e2e_test_coverage_breakdown": {"e2e_test_coverage_statements": $e2e_test_coverage_statements, "e2e_test_coverage_branches": $e2e_test_coverage_branches, "e2e_test_coverage_functions": $e2e_test_coverage_functions, "e2e_test_coverage_lines": $e2e_test_coverage_lines}, "e2e_test_coverage": $e2e_test_coverage}}')

          echo "$updated_report" > audired-cypress-report.json

      - name: Upload report.json
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: audired-cypress-report
          path: audired-cypress-report.json
