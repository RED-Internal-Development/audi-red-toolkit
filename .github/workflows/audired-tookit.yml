name: Audi RED Toolkit

on:
  workflow_call:
    inputs:
      source_file:
        description: "Source file from the origin directory"
        required: true
        type: string
      destination_repo:
        description: "Destination repository"
        type: string
        required: false
        default: "RED-Internal-Development/audi-red-documentation"
      destination_folder:
        description: "Directory to push the file to"
        type: string
        required: false
      user_email:
        description: "Email for the git commit"
        type: string
        required: true
      user_name:
        description: "GitHub username for the commit"
        type: string
        required: true
      user_actor:
        description: "GitHub username that trigged the pipeline"
        type: string
        required: true
      destination_branch:
        description: "branch to push file to, defaults to main"
        type: string
        required: false
      destination_branch_create:
        description: "Destination branch to create for this commit"
        type: string
        required: false
      commit_message:
        description: "A custom message for the commit"
        type: string
        required: false
      rename:
        description: "Rename the destination file"
        type: string
        required: false
      use_rsync:
        description: "Copy files/directories using rsync instead of cp. Experimental feature, please know your use case"
        type: string
        required: false
      git_server:
        description: "Git server host, default github.com"
        type: string
        required: false
        default: github.com
      msiParentPageIds:
        description: "By default we deployment all feature apps to a documentation sync parent id in msi. If you prefer to deploy elsewhere in this space, list ids"
        default: ""
        type: string
        required: false
      enable_doc_sync:
        description: "Enable doc sync step, DOC_SYNC_KEY is required in secrets"
        type: boolean
        required: true
      enable_scanoss:
        description: "Enable scanoss step, SCANOSS_KEY is required in secrets"
        type: boolean
        required: false
    secrets:
      DOC_SYNC_KEY:
        required: false
        description: "Team key used to copy files to audired"
env:
  NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  doc_sync:
    runs-on: ubuntu-22.04
    if: ${{ always() && inputs.enable_doc_sync }}
    needs: data_sync
    steps:
      - name: Check if branch exists
        id: check_branch
        shell: bash
        run: |
          BRANCH_NAME="${{ inputs.destination_branch }}"
          DESTINATION_REPO="${{ inputs.destination_repo }}"
          API_URL="https://api.github.com/repos/${DESTINATION_REPO}/branches/${BRANCH_NAME}"

          # Query the GitHub API
          response=$(curl -s -o /dev/null -w "%{http_code}" -H "Authorization: Bearer ${{ secrets.DOC_SYNC_KEY }}" "$API_URL")

          if [ "$response" -eq 200 ]; then
              echo "Branch '${BRANCH_NAME}' exists."
              echo "branch_exists=true" >> $GITHUB_OUTPUT
          else
              echo "Branch '${BRANCH_NAME}' does not exist."
              echo "branch_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Checkout
        uses: actions/checkout@v2

      - name: Exclude changelog markdown files from docs sync
        run: |
          find "${{ inputs.source_file }}" -type f -name "CHANGELOG*.md" -print -delete


      - name: Normalize markdown syntax for MDX compatibility
        shell: bash
        run: |
          set -euo pipefail
          source_path="${{ inputs.source_file }}"

          normalize_file() {
            local file="$1"
            python3 - "$file" <<'PY'
          import pathlib
          import re
          import sys

          path = pathlib.Path(sys.argv[1])
          content = path.read_text(encoding="utf-8")

          email_re = re.compile(r"<([A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,})>")
          placeholder_re = re.compile(r"<<<([^>\n]+)>>>")
          angle_re = re.compile(r"<([^>\n]+)>")
          inline_code_split_re = re.compile(r"(`[^`]*`)")


          def normalize_non_code_segment(segment: str) -> str:
              segment = email_re.sub(lambda m: f"[{m.group(1)}](mailto:{m.group(1)})", segment)
              segment = placeholder_re.sub(lambda m: f"&lt;&lt;&lt;{m.group(1)}&gt;&gt;&gt;", segment)

              def angle_repl(match: re.Match[str]) -> str:
                  inner = match.group(1).strip()
                  if inner.startswith(("http://", "https://", "mailto:")):
                      return match.group(0)
                  return f"`<{inner}>`"

              return angle_re.sub(angle_repl, segment)


          lines = content.splitlines(keepends=True)
          in_fence = False
          changed = False

          for idx, line in enumerate(lines):
              stripped = line.lstrip()
              if stripped.startswith("```") or stripped.startswith("~~~"):
                  in_fence = not in_fence
                  continue

              if in_fence:
                  continue

              parts = inline_code_split_re.split(line)
              if len(parts) == 1:
                  new_line = normalize_non_code_segment(line)
              else:
                  new_parts = []
                  for part in parts:
                      if part.startswith("`") and part.endswith("`"):
                          new_parts.append(part)
                      else:
                          new_parts.append(normalize_non_code_segment(part))
                  new_line = "".join(new_parts)

              if new_line != line:
                  lines[idx] = new_line
                  changed = True

          if changed:
              path.write_text("".join(lines), encoding="utf-8")
          PY
          }

          if [ -d "$source_path" ]; then
            find "$source_path" -type f \( -name "*.md" -o -name "*.mdx" \) -print0 | while IFS= read -r -d '' file; do
              normalize_file "$file"
            done
          elif [ -f "$source_path" ]; then
            normalize_file "$source_path"
          else
            echo "Skipping normalization. source_file '$source_path' not found."
          fi


      - name: Eslint check for Docusaurus build compatibility
        shell: bash
        run: |
          set -euo pipefail
          source_dir="${{ inputs.source_file }}"
          mdx_check_dir="$(mktemp -d)"

          if [ -d "$source_dir" ]; then
            if [ "$source_dir" = "." ]; then
              # Keep MDX checker scope aligned with the files we actually sync.
              rsync -av --prune-empty-dirs \
                --include='*/' \
                --include='docs/***' \
                --include='**/Docs/***' \
                --include='**/*.docc/***' \
                --include='**/structurizr/***' \
                --include='*.md' \
                --include='*.mdx' \
                --exclude='*' \
                "$source_dir"/ "$mdx_check_dir"/
            else
              rsync -av --prune-empty-dirs "$source_dir"/ "$mdx_check_dir"/
            fi
          elif [ -f "$source_dir" ]; then
            relative_dir=$(dirname "$source_dir")
            mkdir -p "$mdx_check_dir/$relative_dir"
            cp "$source_dir" "$mdx_check_dir/$source_dir"
          else
            echo "Unsupported source_file: '$source_dir'. Expected a directory or file."
            exit 1
          fi

          cd "$mdx_check_dir"
          npx docusaurus-mdx-checker

      - name: Install mermaid CLI for parsing
        run: npm install -g @mermaid-js/mermaid-cli

      - name: Validate mermaid.js code can be parsed
        run: |
          mkdir -p diagrams
          folder=${{ inputs.source_file }}
          index=1
          find "$folder" -type f \( -name "*.md" -o -name "*.mdx" \) -print0 | while IFS= read -r -d '' file; do
            echo "Processing markdown file: $file"
            
            in_code_block=false
            mermaid_code=""
            temp_file=$(mktemp)
            TMPDIR=$(mktemp -d)

            while IFS= read -r line; do
              if [[ "$line" == '```mermaid' ]]; then
                in_code_block=true
                mermaid_code=""
                continue
              elif [[ "$line" == '```' && "$in_code_block" == true ]]; then
                echo "Found Mermaid diagram:"
                echo "$mermaid_code"
                
                # Write the Mermaid code to a temporary .mmd file and generate the SVG
                diagram_name="diagram_${index}.mmd"
                echo "$mermaid_code" > "diagrams/${diagram_name}"

                if ! mmdc -i "diagrams/${diagram_name}" -o "$TMPDIR/output_${index}.svg" 2> "$TMPDIR/mmdc_error.log"; then
                  echo "❌ Mermaid validation failed in file:"
                  cat "$TMPDIR/mmdc_error.log"
                  exit 1
                fi
                
                in_code_block=false
                index=$((index + 1))
              elif [[ "$in_code_block" == true ]]; then
                mermaid_code="$mermaid_code$line"$'\n'
              else
                echo "$line" >> "$temp_file"
              fi
            done < "$file"
          done
          echo "✅ All Mermaid blocks are valid!"

      - name: Create images from structurizr dsl files
        uses: RED-Internal-Development/audired_structurizr_action@main

      - name: Check if referenced images exist
        run: |
          find docs/ -type f \( -name "*.md" -o -name "*.mdx" \) -print0 | while IFS= read -r -d '' md_file; do
                echo "Checking file: $md_file"
                
                grep -oP '!\[.*?\]\(\K(.*?)(?=\))' "$md_file" | while read -r image; do
                    image=$(echo "$image" | sed 's/[?#].*$//')
                    if [[ "$image" =~ ^https?:// ]]; then
                        echo "Skipping external image: $image"
                        continue
                    elif [[ "$image" =~ ^/ ]]; then
                        IMAGE_PATH="$GITHUB_WORKSPACE$image"
                    else
                        IMAGE_PATH="$(dirname "$md_file")/$image"
                    fi

                    IMAGE_PATH=$(realpath "$IMAGE_PATH")

                    if [[ ! -f "$IMAGE_PATH" ]]; then
                        echo "Image '$image' referenced in '$md_file' does not exist at '$IMAGE_PATH'."
                        exit 1
                    fi
                done
            done

            echo "All images are properly referenced and exist!"

      - name: Prepare documentation sync source
        id: prepare_sync_source
        shell: bash
        run: |
          set -euo pipefail
          source_dir="${{ inputs.source_file }}"
          sync_source_dir="docsync_source"

          rm -rf "$sync_source_dir"
          mkdir -p "$sync_source_dir"

          if [ -d "$source_dir" ]; then
            if [ "$source_dir" = "." ]; then
              # When syncing from repo root, copy only documentation paths (all file types).
              rsync -av --prune-empty-dirs \
                --include='*/' \
                --include='docs/***' \
                --include='**/Docs/***' \
                --include='**/*.docc/***' \
                --include='**/structurizr/***' \
                --include='*.md' \
                --include='*.mdx' \
                --exclude='*' \
                "$source_dir"/ "$sync_source_dir"/
            else
              # For explicit doc source directories, keep all file types.
              rsync -av --prune-empty-dirs "$source_dir"/ "$sync_source_dir"/
            fi
          elif [ -f "$source_dir" ]; then
            relative_dir=$(dirname "$source_dir")
            mkdir -p "$sync_source_dir/$relative_dir"
            cp "$source_dir" "$sync_source_dir/$source_dir"
          else
            echo "Unsupported source_file: '$source_dir'. Expected a directory or file."
            exit 1
          fi

          echo "sync_source=$sync_source_dir/." >> "$GITHUB_OUTPUT"

      - name: Pushes files from Feature App to Audi RED Portal for syndication
        uses: RED-Internal-Development/audred_docsync_action@main
        env:
          API_TOKEN_GITHUB: ${{ secrets.DOC_SYNC_KEY }}
        with:
          source_file: ${{ steps.prepare_sync_source.outputs.sync_source }}
          destination_repo: ${{ inputs.destination_repo }}
          destination_folder: ${{ inputs.destination_folder }}
          destination_branch: ${{ inputs.destination_branch }}
          user_email: ${{ inputs.user_email }}
          user_name: ${{ inputs.user_name }}
          user_actor: ${{ github.actor }}
          use_rsync: true
          destination_branch_exists: ${{ steps.check_branch.outputs.branch_exists }}

  data_sync:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout AudiRed Doc Sync repository
        uses: actions/checkout@v3
        with:
          repository: ${{ inputs.destination_repo }}
          token: ${{ secrets.DOC_SYNC_KEY }}
          ref: doc-sync-queue   

      - name: Dynamically create docs destination folder variables
        id: create-docs-folders
        run: |
          app_name=$(basename ${{ inputs.source_file }})
          docs_branch="${{ inputs.destination_branch }}"
          docs_destination_team_folder="docs/feature_apps/${{ inputs.destination_branch }}"
          docs_destination_app_folder="$docs_destination_team_folder/$app_name"

          echo "docs_destination_team_folder=$docs_destination_team_folder" >> $GITHUB_OUTPUT
          echo "docs_branch=$docs_branch" >> $GITHUB_ENV
          echo "docs_destination_team_folder=$docs_destination_team_folder" >> $GITHUB_ENV
          echo "docs_destination_app_folder=$docs_destination_app_folder" >> $GITHUB_ENV

      - name: Update data/report.json with additional metadata
        run: |
          data_folder="data"
          collection_report_file_path="collection-report/report.json"
          mkdir -p "collection-report"
          echo "{}" > $collection_report_file_path
          report_artifact_json=$(cat $collection_report_file_path)

          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          updated_report_artifact_json=$(echo "$report_artifact_json" | \
            jq --arg timestamp "$timestamp" '.timestamp = $timestamp')
          echo "$updated_report_artifact_json" > collection-report/report.json

          mkdir -p "$data_folder"
          if [ ! -f "$data_folder"/report.json ]; then
              echo "{}" > "$data_folder"/report.json
          fi
          jq --argjson new_report "$updated_report_artifact_json" \
            '. * $new_report' "$data_folder"/report.json > tmp_report.json
          mv tmp_report.json "$data_folder"/report.json

      - name: Upload final report
        if: github.event.workflow_run.id != ''
        uses: actions/upload-artifact@v4
        with:
          name: data-report
          path: collection-report/report.json

      - name: Commit and push changes
        run: |
          git config --global user.email ${{ inputs.user_email }}
          git config --global user.name ${{ inputs.user_name }}
          git add data/report.json
          git add deployment/
          if ! git diff-index --quiet HEAD; then
              echo "commiting changes"
              git commit -m "Update report.json and deployment config from ${{ inputs.destination_branch }}"
              git push
          else
              echo "No changes to push"
          fi
