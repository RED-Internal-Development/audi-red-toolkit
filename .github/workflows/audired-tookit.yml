name: Audi RED Toolkit

on:
  workflow_call:
    inputs:
      source_file:
        description: "Source file from the origin directory"
        required: true
        type: string
      destination_repo:
        description: "Destination repository"
        type: string
        required: false
        default: "RED-Internal-Development/audi-red-documentation"
      destination_folder:
        description: "Directory to push the file to"
        type: string
        required: false
      user_email:
        description: "Email for the git commit"
        type: string
        required: true
      user_name:
        description: "GitHub username for the commit"
        type: string
        required: true
      user_actor:
        description: "GitHub username that trigged the pipeline"
        type: string
        required: true
      destination_branch:
        description: "[Deprecated] Branch to push file to. Branch is now selected from app-type profile."
        type: string
        required: false
      destination_branch_create:
        description: "[Deprecated] Ignored. Branch selection is profile-driven by app type."
        type: string
        required: false
      commit_message:
        description: "A custom message for the commit"
        type: string
        required: false
      rename:
        description: "Rename the destination file"
        type: string
        required: false
      use_rsync:
        description: "Copy files/directories using rsync instead of cp. Experimental feature, please know your use case"
        type: string
        required: false
      git_server:
        description: "Git server host, default github.com"
        type: string
        required: false
        default: github.com
      msiParentPageIds:
        description: "[Deprecated] Ignored. MSI parent placement is profile-driven by app type."
        default: ""
        type: string
        required: false
      enable_data_sync:
        description: "Enable data sync step"
        type: boolean
        required: false
        default: true
      enable_doc_sync:
        description: "Enable doc sync step, DOC_SYNC_KEY is required in secrets"
        type: boolean
        required: true
      enable_scanoss:
        description: "[Deprecated] Ignored. Kept for backward compatibility."
        type: boolean
        required: false
        default: false
      enable_vwgoa_prod_support_deployment:
        description: "Enable app deployment to VWGOA Production Support Space"
        type: boolean
        required: false
        default: false
      app_type:
        description: "Preferred profile key. Supported values: feature_app, backend_service, mobile_app, graph_service, special_app."
        type: string
        required: false
      project_type:
        description: "[Deprecated] Legacy type input. Mapped to app_type when provided and app_type is not set."
        type: string
        required: false
        default: "auto-detect"
      red_docs_portal_path_override:
        description: "Optional RED docs portal path override for special-app only. Must start with docs/"
        type: string
        required: false
    secrets:
      DOC_SYNC_KEY:
        required: false
        description: "Team key used to copy files to audired"
      SCANOSS_KEY:
        required: false
        description: "[Deprecated] Ignored. Keeping for backward compatibility."
env:
  NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  detect_project:
    runs-on: ubuntu-latest
    outputs:
      app_type: ${{ steps.detect.outputs.app_type }}
      project_type: ${{ steps.detect.outputs.project_type }}
      has_jest: ${{ steps.detect.outputs.has_jest }}
      has_cypress: ${{ steps.detect.outputs.has_cypress }}
      docs_base_path: ${{ steps.detect.outputs.docs_base_path }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Detect project type
        id: detect
        shell: bash
        run: |
          INPUT_APP_TYPE="${{ inputs.app_type }}"
          INPUT_TYPE="${{ inputs.project_type }}"

          HAS_CYPRESS=false
          HAS_JEST=false
          DETECTION_MODE=""
          RESOLVED_APP_TYPE=""
          DOCS_PATH="feature_apps"

          map_legacy_project_type() {
            case "$1" in
              frontend-app|frontend_app) echo "feature_app" ;;
              backend-service|backend_service) echo "backend_service" ;;
              mobile-app|mobile_app) echo "mobile_app" ;;
              graph-service|graph_service) echo "graph_service" ;;
              special-app|special_app) echo "special_app" ;;
              *) echo "" ;;
            esac
          }

          validate_app_type() {
            case "$1" in
              feature_app|backend_service|mobile_app|graph_service|special_app) return 0 ;;
              *) return 1 ;;
            esac
          }

          # app_type takes precedence when set
          if [[ -n "$INPUT_APP_TYPE" ]]; then
            if ! validate_app_type "$INPUT_APP_TYPE"; then
              echo "::error::Invalid app_type '$INPUT_APP_TYPE'. Supported: feature_app, backend_service, mobile_app, graph_service, special_app"
              exit 1
            fi
            RESOLVED_APP_TYPE="$INPUT_APP_TYPE"
            DETECTION_MODE="app_type"
          elif [[ "$INPUT_TYPE" != "auto-detect" ]]; then
            RESOLVED_APP_TYPE="$(map_legacy_project_type "$INPUT_TYPE")"
            if [[ -z "$RESOLVED_APP_TYPE" ]]; then
              echo "::error::Invalid project_type '$INPUT_TYPE'. Use app_type instead."
              exit 1
            fi
            DETECTION_MODE="project_type (deprecated)"
          else
            # Auto-detect based on package.json and file structure
            DETECTION_MODE="auto-detect (deprecated)"
            echo "::warning::Using auto-detect is deprecated. Please provide app_type."
            
            if [ -f "package.json" ]; then
              # Check for Cypress
              if jq -e '.scripts | to_entries[] | select(.value | contains("cypress"))' package.json > /dev/null 2>&1; then
                HAS_CYPRESS=true
              fi
              
              if [ -d "cypress" ]; then
                HAS_CYPRESS=true
              fi
              
              # Check for Jest
              if jq -e '.scripts | to_entries[] | select(.key | test("test|jest"))' package.json > /dev/null 2>&1; then
                HAS_JEST=true
              fi
              
              if [ -f "jest.config.js" ] || [ -f "jest.config.ts" ] || jq -e '.jest' package.json > /dev/null 2>&1; then
                HAS_JEST=true
              fi
            fi
            
            # Determine app_type
            if [[ "$HAS_CYPRESS" == "true" ]]; then
              RESOLVED_APP_TYPE="feature_app"
            elif [[ "$HAS_JEST" == "true" ]]; then
              RESOLVED_APP_TYPE="backend_service"
            else
              RESOLVED_APP_TYPE="feature_app"
            fi
          fi

          case "$RESOLVED_APP_TYPE" in
            backend_service) DOCS_PATH="backend_services" ;;
            special_app) DOCS_PATH="special_apps" ;;
            *) DOCS_PATH="feature_apps" ;;
          esac

          # Keep both outputs for backward compatibility; both carry normalized app_type.
          echo "app_type=$RESOLVED_APP_TYPE" >> $GITHUB_OUTPUT
          echo "project_type=$RESOLVED_APP_TYPE" >> $GITHUB_OUTPUT
          echo "docs_base_path=$DOCS_PATH" >> $GITHUB_OUTPUT
          
          # Always output test framework availability
          echo "has_cypress=$HAS_CYPRESS" >> $GITHUB_OUTPUT
          echo "has_jest=$HAS_JEST" >> $GITHUB_OUTPUT
          
          echo "âœ… Resolved app_type: $RESOLVED_APP_TYPE"
          echo "âœ… Resolution mode: $DETECTION_MODE"
          echo "âœ… Cypress available: $HAS_CYPRESS"
          echo "âœ… Jest available: $HAS_JEST"
          echo "âœ… Docs base path: $DOCS_PATH"

  doc_sync:
    runs-on: ubuntu-22.04
    if: ${{ always() && inputs.enable_doc_sync }}
    needs: [detect_project]
    steps:
      - name: Checkout toolkit config
        uses: actions/checkout@v4
        with:
          repository: RED-Internal-Development/audi-red-toolkit
          ref: simplified-docs-sync
          token: ${{ secrets.GITHUB_TOKEN }}
          path: toolkit-config

      - name: Resolve docs sync target from profile
        id: resolve_target
        shell: bash
        run: |
          app_name=$(basename "${{ github.repository }}")
          project_type="${{ needs.detect_project.outputs.project_type }}"
          profile_file="toolkit-config/config/app-type-profiles.yml"

          if [[ ! -f "$profile_file" ]]; then
            echo "Profile file not found: $profile_file"
            exit 1
          fi

          profile_key=$(ruby -ryaml -e '
            data = YAML.load_file(ARGV[0])
            raw = ARGV[1].to_s.strip
            normalized = raw.tr("-", "_")
            alias_map = {
              "frontend_app" => "feature_app",
              "backend_service" => "backend_service",
              "mobile_app" => "mobile_app",
              "graph_service" => "graph_service",
              "special_app" => "special_app"
            }
            candidate = alias_map.fetch(normalized, normalized)
            unless data.key?(candidate)
              warn "Unsupported project_type/profile mapping: #{raw} -> #{candidate}"
              exit 1
            end
            puts candidate
          ' "$profile_file" "$project_type")

          base_path=$(ruby -ryaml -e 'data = YAML.load_file(ARGV[0]); puts data[ARGV[1]]["red_docs"]["base_path"]' "$profile_file" "$profile_key")
          docs_branch=$(ruby -ryaml -e 'data = YAML.load_file(ARGV[0]); puts data[ARGV[1]]["red_docs"]["branch"]' "$profile_file" "$profile_key")
          docs_destination_app_folder=$(echo "$base_path" | sed "s|{app}|$app_name|g")

          if [[ "$profile_key" == "special_app" ]]; then
            custom_path="${{ inputs.red_docs_portal_path_override }}"
            if [[ -n "$custom_path" ]]; then
              if [[ "$custom_path" != docs/* ]]; then
                echo "::error::Invalid red_docs_portal_path_override. It must start with 'docs/'"
                exit 1
              fi
              if [[ "$custom_path" == *".."* ]]; then
                echo "::error::Invalid red_docs_portal_path_override. '..' is not allowed."
                exit 1
              fi
              custom_path="${custom_path%/}"
              if [[ "$custom_path" == *"{app}"* ]]; then
                docs_destination_app_folder=$(echo "$custom_path" | sed "s|{app}|$app_name|g")
              else
                docs_destination_app_folder="$custom_path/$app_name"
              fi
            fi
          fi

          docs_destination_team_folder=$(dirname "$docs_destination_app_folder")
          echo "app_name=$app_name" >> "$GITHUB_OUTPUT"
          echo "docs_branch=$docs_branch" >> "$GITHUB_OUTPUT"
          echo "docs_destination_team_folder=$docs_destination_team_folder" >> "$GITHUB_OUTPUT"

      - name: Check if branch exists
        id: check_branch
        shell: bash
        run: |
          BRANCH_NAME="${{ steps.resolve_target.outputs.docs_branch }}"
          DESTINATION_REPO="${{ inputs.destination_repo }}"
          API_URL="https://api.github.com/repos/${DESTINATION_REPO}/branches/${BRANCH_NAME}"

          # Query the GitHub API
          response=$(curl -s -o /dev/null -w "%{http_code}" -H "Authorization: Bearer ${{ secrets.DOC_SYNC_KEY }}" "$API_URL")

          if [ "$response" -eq 200 ]; then
              echo "Branch '${BRANCH_NAME}' exists."
              echo "branch_exists=true" >> $GITHUB_OUTPUT
          else
              echo "Branch '${BRANCH_NAME}' does not exist."
              echo "branch_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Checkout
        uses: actions/checkout@v4

      - name: Eslint check for Docusaurus build compatibility
        run: |
          cd ${{ inputs.source_file }}
          npx docusaurus-mdx-checker

      - name: Install mermaid CLI for parsing
        run: npm install -g @mermaid-js/mermaid-cli

      - name: Validate mermaid.js code can be parsed
        run: |
          mkdir -p diagrams
          folder=${{ inputs.source_file }}
          index=1
          find "$folder" -type f \( -name "*.md" -o -name "*.mdx" \) -print0 | while IFS= read -r -d '' file; do
            echo "Processing markdown file: $file"
            
            in_code_block=false
            mermaid_code=""
            temp_file=$(mktemp)
            TMPDIR=$(mktemp -d)

            while IFS= read -r line; do
              if [[ "$line" == '```mermaid' ]]; then
                in_code_block=true
                mermaid_code=""
                continue
              elif [[ "$line" == '```' && "$in_code_block" == true ]]; then
                echo "Found Mermaid diagram:"
                echo "$mermaid_code"
                
                # Write the Mermaid code to a temporary .mmd file and generate the SVG
                diagram_name="diagram_${index}.mmd"
                echo "$mermaid_code" > "diagrams/${diagram_name}"

                if ! mmdc -i "diagrams/${diagram_name}" -o "$TMPDIR/output_${index}.svg" 2> "$TMPDIR/mmdc_error.log"; then
                  echo "âŒ Mermaid validation failed in file:"
                  cat "$TMPDIR/mmdc_error.log"
                  exit 1
                fi
                
                in_code_block=false
                index=$((index + 1))
              elif [[ "$in_code_block" == true ]]; then
                mermaid_code="$mermaid_code$line"$'\n'
              else
                echo "$line" >> "$temp_file"
              fi
            done < "$file"
          done
          echo "âœ… All Mermaid blocks are valid!"

      - name: Create images from structurizr dsl files
        uses: RED-Internal-Development/audired_structurizr_action@main

      - name: Check if referenced images exist
        run: |
          find docs/ -type f \( -name "*.md" -o -name "*.mdx" \) -print0 | while IFS= read -r -d '' md_file; do
                echo "Checking file: $md_file"
                
                grep -oP '!\[.*?\]\(\K(.*?)(?=\))' "$md_file" | while read -r image; do
                    image=$(echo "$image" | sed 's/[?#].*$//')
                    if [[ "$image" =~ ^https?:// ]]; then
                        echo "Skipping external image: $image"
                        continue
                    elif [[ "$image" =~ ^/ ]]; then
                        IMAGE_PATH="$GITHUB_WORKSPACE$image"
                    else
                        IMAGE_PATH="$(dirname "$md_file")/$image"
                    fi

                    IMAGE_PATH=$(realpath "$IMAGE_PATH")

                    if [[ ! -f "$IMAGE_PATH" ]]; then
                        echo "Image '$image' referenced in '$md_file' does not exist at '$IMAGE_PATH'."
                        exit 1
                    fi
                done
            done

            echo "All images are properly referenced and exist!"

      - name: Download collection report artifact
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: data-report
          path: data-report

      - name: Verify artifact and create fallback if needed
        run: |
          if [ -f "data-report/report.json" ]; then
            echo "âœ… Artifact downloaded successfully"
            ls -la data-report/
          else
            echo "âš ï¸ Artifact not found - creating empty report for backend services"
            mkdir -p data-report
            echo '{}' > data-report/report.json
          fi

      - name: Create or update project metrics with report data
        run: |
          FILE_PATH="${{ inputs.source_file }}/project_metrics.mdx"
          report_file="data-report/report.json"
          report_file_data=$(cat $report_file)

          if [ -z "$report_file_data" ]; then
              echo "No report results to include, skipping"
          else
              repo_name=$(echo "$report_file_data" | jq -r 'to_entries | .[0].key')
              echo "REPO: $repo_name"
              lighthouse_score=$(jq ".[\"$repo_name\"].lighthouse_score" "$report_file")
              echo "lighthouse score: $lighthouse_score"
              unit_test_coverage_data=$(jq ".[\"$repo_name\"].unit_test_coverage" "$report_file")
              echo "unit test coverage: $unit_test_coverage_data"
              e2e_test_coverage=$(jq ".[\"$repo_name\"].e2e_test_coverage" "$report_file")
              echo "e2e test coverage: $e2e_test_coverage"

              NEW_CONTENT="# CI Report Summary\n"
              if [ -n "$lighthouse_score" ]  && [ "$lighthouse_score" != "null" ]; then
                NEW_CONTENT+="- **Lighthouse Score**: $lighthouse_score / 1\n"
              fi

              if [ -n "$e2e_test_coverage" ] && [ "$e2e_test_coverage" != "null" ]; then
                rounded_e2e_test_coverage=$(printf "%.0f" $e2e_test_coverage)
                NEW_CONTENT+="- **E2E Test Coverage**: $rounded_e2e_test_coverage%\n"
              fi

              if [ -n "$unit_test_coverage_data" ] && [ "$unit_test_coverage_data" != "null" ]; then
                echo "checking unit_test_coverage_data"
                check_value() {
                  local value=$1
                  value=$(echo "$value" | awk '{print $1+0}')
                  
                  if (( $(echo "$value > 80" | bc -l) )); then
                    echo ":white_check_mark:"
                  elif (( $(echo "$value > 50" | bc -l) )); then
                    echo ":warning:"
                  else
                    echo ":x:"
                  fi
                }
                
                unit_statements=$(jq ".[\"$repo_name\"].unit_test_coverage_data.statement_coverage" "$report_file")
                unit_functions=$(jq ".[\"$repo_name\"].unit_test_coverage_data.function_coverage" "$report_file")
                unit_branches=$(jq ".[\"$repo_name\"].unit_test_coverage_data.branch_coverage" "$report_file")
                unit_lines=$(jq ".[\"$repo_name\"].unit_test_coverage_data.line_coverage" "$report_file")
                unit_average=$(jq ".[\"$repo_name\"].unit_test_coverage_data.average_coverage" "$report_file")
                echo "unit_average: $unit_average"
                rounded_unit_test_coverage=$(printf "%.0f" $unit_average)
                echo "rounded_unit_test_coverage: $rounded_unit_test_coverage"
                NEW_CONTENT+=$(
                  echo "## Unit Test Coverage" $'\n' \
                      "| Category | Coverage | Rating |" $'\n' \
                      "|-------------|------------|----------------|" $'\n' \
                      "| Statements | "$unit_statements%" | $(check_value $unit_statements) |" $'\n' \
                      "| Functions  | "$unit_functions%" | $(check_value $unit_functions) |" $'\n' \
                      "| Branches   | "$unit_branches%" | $(check_value $unit_branches) |" $'\n' \
                      "| Lines      | "$unit_lines%" | $(check_value $unit_lines) |" $'\n' \
                      "| Overall    | "$rounded_unit_test_coverage%" | $(check_value $rounded_unit_test_coverage) |" $'\n'
                )

                RECOMMENDATIONS_CONTENT=""
                if (( $(echo "$unit_branches < 60" | bc -l) )); then
                  RECOMMENDATIONS_CONTENT+="- Test more branches (if/else, error handling)"
                fi

                if (( $(echo "$unit_functions < 70" | bc -l) )); then
                  RECOMMENDATIONS_CONTENT+="- Write additional unit tests for functions."
                fi

                if (( $(echo "$unit_lines < 70" | bc -l) )); then
                  RECOMMENDATIONS_CONTENT+="- Better cover the entire code (e.g. rare code paths)"
                fi

                if [[ -n "RECOMMENDATIONS_CONTENT" ]]; then
                  NEW_CONTENT+=$(
                    echo $'\n' "### :pushpin: Recommendations:" $'\n' \
                        "$RECOMMENDATIONS_CONTENT" $'\n'
                  )
                fi
              elif [ -n "$unit_test_coverage" ] && [ "$unit_test_coverage" != "null" ]; then
                rounded_unit_test_coverage=$(printf "%.0f" $unit_test_coverage)
                NEW_CONTENT+="- **Unit Test Coverage**: $rounded_unit_test_coverage%\n"
              fi

              # Check if the file exists
              if [ -f "$FILE_PATH" ]; then
                echo "File exists. Appending content to the top of the file: $FILE_PATH"
                echo -e "$NEW_CONTENT\n$(cat $FILE_PATH)" > $FILE_PATH
              else
                echo "File does not exist. Creating a new file: $FILE_PATH"
                echo -e "$NEW_CONTENT" > $FILE_PATH
              fi
          fi

      - name: Prepare source path for rsync
        id: prepare_source
        run: |
          SOURCE="${{ inputs.source_file }}"
          # Add trailing slash if source is a directory (needed for rsync to copy contents, not the folder itself)
          if [ -d "$SOURCE" ]; then
            SOURCE="${SOURCE%/}/"
          fi
          echo "source_path=$SOURCE" >> $GITHUB_OUTPUT
          echo "Using source path: $SOURCE"

      - name: Push documentation to Audi RED Portal for syndication
        uses: RED-Internal-Development/audred_docsync_action@main
        env:
          API_TOKEN_GITHUB: ${{ secrets.DOC_SYNC_KEY }}
        with:
          source_file: ${{ steps.prepare_source.outputs.source_path }}
          destination_repo: ${{ inputs.destination_repo }}
          destination_folder: ${{ steps.resolve_target.outputs.docs_destination_team_folder || inputs.destination_folder }}
          destination_branch: ${{ steps.resolve_target.outputs.docs_branch }}
          rename: ${{ steps.resolve_target.outputs.app_name || inputs.rename }}
          user_email: ${{ inputs.user_email }}
          user_name: ${{ inputs.user_name }}
          user_actor: ${{ github.actor }}
          use_rsync: true
          destination_branch_exists: ${{ steps.check_branch.outputs.branch_exists }}

  data_sync:
    runs-on: ubuntu-latest
    if: ${{ always() && inputs.enable_data_sync }}
    needs: [detect_project, cypress_sync, jest_sync]
    outputs:
      docs_destination_team_folder: ${{ steps.create-docs-folders.outputs.docs_destination_team_folder }}
      docs_destination_app_folder: ${{ steps.create-docs-folders.outputs.docs_destination_app_folder }}
      app_name: ${{ steps.create-docs-folders.outputs.app_name }}
      docs_branch: ${{ steps.create-docs-folders.outputs.docs_branch }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Collection of metadata from repository
        id: metadata_collection
        uses: RED-Internal-Development/audi-red-app-metadata-kit@staging
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          repository: ${{ github.repository }}
          workflow_run_id: ${{ github.event.workflow_run.id }}

      - name: Checkout AudiRed Doc Sync repository
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.destination_repo }}
          token: ${{ secrets.DOC_SYNC_KEY }}
          ref: doc-sync-queue

      - name: Checkout toolkit config
        uses: actions/checkout@v4
        with:
          repository: RED-Internal-Development/audi-red-toolkit
          ref: simplified-docs-sync
          token: ${{ secrets.GITHUB_TOKEN }}
          path: toolkit-config

      - name: Download metadata artifact
        uses: actions/download-artifact@v4
        with:
          name: metadata-report
          path: metadata-report

      - name: Download cypress report artifact
        if: github.event_name == 'schedule' && needs.cypress_sync.outputs.cypress_run == 'true'
        uses: actions/download-artifact@v4
        with:
          name: audired-cypress-report
          path: audired-cypress-report

      - name: Download jest report artifact
        if: |
          (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch') &&
          needs.jest_sync.outputs.jest_run == 'true'
        uses: actions/download-artifact@v4
        with:
          name: audired-jest-report
          path: audired-jest-report

      - name: Download collection report artifact
        if: github.event.workflow_run.id != ''
        uses: actions/download-artifact@v4
        with:
          name: audired-collection-report
          path: collection-report
          github-token: ${{ github.token }}
          repository: ${{ github.repository }}
          run-id: ${{ github.event.workflow_run.id }}

      - name: Dynamically create docs destination folder variables
        id: create-docs-folders
        run: |
          app_name=$(basename "${{ github.repository }}")
          project_type="${{ needs.detect_project.outputs.project_type }}"
          profile_file="toolkit-config/config/app-type-profiles.yml"
          
          if [[ ! -f "$profile_file" ]]; then
            echo "Profile file not found: $profile_file"
            exit 1
          fi

          # Normalize project_type and map legacy aliases to profile keys.
          # Fail fast when project_type is not backed by a profile key.
          profile_key=$(ruby -ryaml -e '
            data = YAML.load_file(ARGV[0])
            raw = ARGV[1].to_s.strip
            normalized = raw.tr("-", "_")
            alias_map = {
              "frontend_app" => "feature_app",
              "backend_service" => "backend_service",
              "mobile_app" => "mobile_app",
              "graph_service" => "graph_service",
              "special_app" => "special_app"
            }
            candidate = alias_map.fetch(normalized, normalized)
            unless data.key?(candidate)
              warn "Unsupported project_type/profile mapping: #{raw} -> #{candidate}"
              exit 1
            end
            puts candidate
          ' "$profile_file" "$project_type")

          base_path=$(ruby -ryaml -e 'data = YAML.load_file(ARGV[0]); puts data[ARGV[1]]["red_docs"]["base_path"]' "$profile_file" "$profile_key")
          docs_branch=$(ruby -ryaml -e 'data = YAML.load_file(ARGV[0]); puts data[ARGV[1]]["red_docs"]["branch"]' "$profile_file" "$profile_key")
          docs_destination_app_folder=$(echo "$base_path" | sed "s|{app}|$app_name|g")
          
          if [[ "$profile_key" == "special_app" ]]; then
            custom_path="${{ inputs.red_docs_portal_path_override }}"
            if [[ -n "$custom_path" ]]; then
              if [[ "$custom_path" != docs/* ]]; then
                echo "::error::Invalid red_docs_portal_path_override. It must start with 'docs/'"
                exit 1
              fi
              if [[ "$custom_path" == *".."* ]]; then
                echo "::error::Invalid red_docs_portal_path_override. '..' is not allowed."
                exit 1
              fi
              custom_path="${custom_path%/}"
              if [[ "$custom_path" == *"{app}"* ]]; then
                docs_destination_app_folder=$(echo "$custom_path" | sed "s|{app}|$app_name|g")
              else
                docs_destination_app_folder="$custom_path/$app_name"
              fi
            fi
          fi

          docs_destination_team_folder=$(dirname "$docs_destination_app_folder")

          echo "docs_destination_team_folder=$docs_destination_team_folder" >> $GITHUB_OUTPUT
          echo "docs_destination_app_folder=$docs_destination_app_folder" >> $GITHUB_OUTPUT
          echo "app_name=$app_name" >> $GITHUB_OUTPUT
          echo "docs_branch=$docs_branch" >> $GITHUB_OUTPUT
          echo "docs_branch=$docs_branch" >> $GITHUB_ENV
          echo "docs_destination_team_folder=$docs_destination_team_folder" >> $GITHUB_ENV
          echo "docs_destination_app_folder=$docs_destination_app_folder" >> $GITHUB_ENV
          
          echo "ðŸ“ Project type: $project_type"
          echo "ðŸ“ Profile key: $profile_key"
          echo "ðŸ“ Docs destination folder: $docs_destination_team_folder"
          echo "ðŸ“ App/Service folder: $docs_destination_app_folder"

      - name: Update data/report.json with additional metadata
        run: |
          data_folder="data"
          per_repo_folder="$data_folder/per-repo"
          collection_report_file_path="collection-report/report.json"
          metadata_file_path="metadata-report/metadata-report.json"
          cypress_data_file_path="audired-cypress-report/audired-cypress-report.json"
          output_file_path="combined-data.json"

          repo_name=$(jq -r 'to_entries | .[0].key' "$metadata_file_path")
          mkdir -p "collection-report"

          if [[ -n "${{ github.event.workflow_run.id }}" && -f "$collection_report_file_path" ]]; then
            jq --arg repo_name "$repo_name" \
              '.[ $repo_name ] |= (. + (input | .[ $repo_name ]))' \
              "$collection_report_file_path" "$metadata_file_path" > "$output_file_path"
          else
            cp "$metadata_file_path" "$output_file_path"
          fi

          if [[ -f "$cypress_data_file_path" ]]; then
            jq --arg repo_name "$repo_name" \
              '.[ $repo_name ] |= (. + (input | .[ $repo_name ]))' \
              "$output_file_path" "$cypress_data_file_path" > "$output_file_path.tmp"
            mv "$output_file_path.tmp" "$output_file_path"
          else
            echo "No cypress data artifact found."
          fi

          # Merge Jest data from Pattern A (scheduled runs)
          jest_data_file_path="audired-jest-report/audired-jest-report.json"
          if [[ -f "$jest_data_file_path" ]]; then
            echo "Merging Jest coverage data from scheduled run..."
            jq --arg repo_name "$repo_name" \
              '.[ $repo_name ] |= (. + (input | .[ $repo_name ]))' \
              "$output_file_path" "$jest_data_file_path" > "$output_file_path.tmp"
            mv "$output_file_path.tmp" "$output_file_path"
            echo "âœ… Jest data merged successfully"
          else
            echo "No Jest data from scheduled run to merge"
          fi

          # prod_support_enabled variable

          prod_support_enabled=false
          if [ "${{ inputs.enable_vwgoa_prod_support_deployment }}" == "true" ]; then
              prod_support_enabled=true
          fi

          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          jq --arg repo_name "$repo_name" \
            --argjson prod_support "$prod_support_enabled" \
            --arg docs_destination_team_folder "$docs_destination_team_folder" \
            --arg docs_destination_app_folder "$docs_destination_app_folder" \
            --arg docs_branch "$docs_branch" \
            --arg timestamp "$timestamp" \
            '.[$repo_name] |= . + {prod_support_enabled: $prod_support, docs: {docs_destination_team_folder: $docs_destination_team_folder, docs_destination_app_folder: $docs_destination_app_folder, docs_branch: $docs_branch}, timestamp: $timestamp}' \
            "$output_file_path" > collection-report/report.json

          # Push artifact report to existing report.json
          mkdir -p "$data_folder"

          # Check if report.json exists inside the $data_folder folder; if not, create it
          if [ ! -f "$data_folder"/report.json ]; then
              echo "{}" > "$data_folder"/report.json
          fi

          jq -s '.[0] * .[1]' "$data_folder"/report.json collection-report/report.json > tmp_report.json

          mv tmp_report.json "$data_folder"/report.json

          mkdir -p "$per_repo_folder"
          safe_repo_name=$(echo "$repo_name" | tr '/@ ' '___')
          per_repo_file="$per_repo_folder/$safe_repo_name.json"
          repo_snapshot_file="$per_repo_file.tmp"
          jq --arg repo_name "$repo_name" '.[$repo_name]' collection-report/report.json > "$repo_snapshot_file"
          if [[ ! -s "$repo_snapshot_file" ]] || [[ "$(cat "$repo_snapshot_file")" = "null" ]]; then
            echo "{}" > "$per_repo_file"
            rm -f "$repo_snapshot_file"
          else
            mv "$repo_snapshot_file" "$per_repo_file"
          fi

      - name: Upload final report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-report
          path: collection-report/report.json
          retention-days: 1
          if-no-files-found: warn

      - name: Create Repo Configuration File for MSI/Cloud Deployments
        run: |
          app_name=$(basename "${{ github.repository }}")
          project_type="${{ needs.detect_project.outputs.project_type }}"
          profile_file="toolkit-config/config/app-type-profiles.yml"

          config_file="deployment/repo_configs/$app_name.json"

          mkdir -p "$(dirname "$config_file")"

          if [[ ! -f "$profile_file" ]]; then
            echo "Profile file not found: $profile_file"
            exit 1
          fi

          profile_key=$(ruby -ryaml -e '
            data = YAML.load_file(ARGV[0])
            raw = ARGV[1].to_s.strip
            normalized = raw.tr("-", "_")
            alias_map = {
              "frontend_app" => "feature_app",
              "backend_service" => "backend_service",
              "mobile_app" => "mobile_app",
              "graph_service" => "graph_service",
              "special_app" => "special_app"
            }
            candidate = alias_map.fetch(normalized, normalized)
            unless data.key?(candidate)
              warn "Unsupported project_type/profile mapping: #{raw} -> #{candidate}"
              exit 1
            end
            puts candidate
          ' "$profile_file" "$project_type")

          if [[ "$profile_key" == "special_app" && "${{ inputs.enable_vwgoa_prod_support_deployment }}" == "true" ]]; then
            echo "::error::VWGOA deployment is not supported for project_type '$project_type' (special_app)."
            exit 1
          fi

          vwgoa_parent_page_id=$(ruby -ryaml -e '
            data = YAML.load_file(ARGV[0])
            profile = data[ARGV[1]] || {}
            vwgoa = profile["vwgoa"] || {}
            puts(vwgoa["parent_page_id"].to_s)
          ' "$profile_file" "$profile_key")
          
          # NOTE: Config files are ONLY created for VWGOA deployments.
          # MSI parent placement is now determined by project_type in the MSI workflow,
          # so per-app MSI config is no longer required.

          if [[ "${{ inputs.enable_vwgoa_prod_support_deployment }}" == "false" ]]; then
            if [[ -f "$config_file" ]]; then
              rm "$config_file"
            fi
          else
            if [[ -z "$vwgoa_parent_page_id" ]]; then
              echo "::error::VWGOA sync is not available for project_type '$project_type' (profile key '$profile_key'). Define 'vwgoa.parent_page_id' in app-type profile to enable."
              exit 1
            fi

            echo '{}' > "$config_file"
            if [ "${{ inputs.enable_vwgoa_prod_support_deployment }}" == "true" ]; then
              vwgoa_enabled=true
              vwgoa_jq_string=".vwgoa_enabled = $vwgoa_enabled | "
            fi
            jq "$vwgoa_jq_string.app_name = \"$app_name\" | .project_type = \"$project_type\" | .vwgoa_parent_page_id = \"$vwgoa_parent_page_id\"" "$config_file" > "$config_file.tmp" && mv "$config_file.tmp" "$config_file"
            echo "config file:"
            cat "$config_file"
          fi

      - name: Commit and push changes
        run: |
          git config --global user.email ${{ inputs.user_email }}
          git config --global user.name ${{ inputs.user_name }}
          git add data/report.json
          if [ -d data/per-repo ]; then
              git add data/per-repo/
          fi
          git add deployment/
          if ! git diff-index --quiet HEAD; then
              echo "commiting changes"
              git commit -m "Update report.json and deployment config from $docs_branch"
              git push
          else
              echo "No changes to push"
          fi

  jest_sync:
    runs-on: ubuntu-latest
    needs: detect_project
    if: |
      needs.detect_project.outputs.has_jest == 'true' &&
      (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')
    outputs:
      jest_run: ${{ steps.check_script.outputs.exists }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Check if Jest test script exists
        id: check_script
        shell: bash
        run: |
          if jq -e '.scripts["test"] or .scripts["test:unit"] or .scripts["test:coverage"]' package.json > /dev/null; then
            echo "Jest test script found"
            echo "exists=true" >> "$GITHUB_OUTPUT"
          else
            echo "No Jest test script found"
            echo "exists=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Setup Node.js
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/setup-node@v4
        with:
          node-version-file: ".nvmrc"
          cache: npm
          registry-url: https://npm.pkg.github.com

      - name: Install dependencies
        if: steps.check_script.outputs.exists == 'true'
        shell: bash
        run: npm ci
        env:
          NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Run Jest with Coverage
        if: steps.check_script.outputs.exists == 'true'
        shell: bash
        run: |
          # Try different script patterns
          if jq -e '.scripts["test:coverage"]' package.json > /dev/null; then
            npm run test:coverage
          elif jq -e '.scripts["test:unit"]' package.json > /dev/null; then
            npm run test:unit -- --coverage
          else
            npm test -- --coverage
          fi

      - name: Collect Jest Coverage using Collection Kit
        if: steps.check_script.outputs.exists == 'true'
        uses: RED-Internal-Development/audi-red-collection-kit@main
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          jest_coverage_file_path: coverage/coverage-summary.json
      
      - name: Rename report for consistency
        if: steps.check_script.outputs.exists == 'true'
        shell: bash
        run: |
          if [ -f "report.json" ]; then
            mv report.json audired-jest-report.json
            echo "âœ… Jest coverage report created via collection-kit"
            cat audired-jest-report.json
          fi

      - name: Upload Jest Coverage Report
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: audired-jest-report
          path: audired-jest-report.json
          retention-days: 30

  cypress_sync:
    runs-on: ubuntu-latest
    needs: detect_project
    if: |
      needs.detect_project.outputs.has_cypress == 'true' &&
      (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')
    outputs:
      cypress_run: ${{ steps.check_script.outputs.exists }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Check if test:e2e-run-dev script exists
        id: check_script
        run: |
          if jq -e '.scripts["test:e2e-run-dev"]' package.json > /dev/null; then
            echo "Script exists. Starting cypress install, run, and coverage collection"
            echo "exists=true" >> "$GITHUB_OUTPUT"
          else
            echo "Script does not exist. Skipping cypress collection"
            echo "exists=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Load Cypress cache
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/Cypress
          key: ${{ runner.os }}-cypress-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-cypress

      - name: Setup node
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/setup-node@v4
        with:
          node-version-file: ".nvmrc"
          cache: npm
          registry-url: https://npm.pkg.github.com

      - name: Install dependencies
        if: steps.check_script.outputs.exists == 'true'
        run: |
          npm ci

      - name: Cypress Coverage Run
        if: steps.check_script.outputs.exists == 'true'
        run: |
          echo "Runs only on a cron job"
          npm run test:e2e-run-dev

      - name: E2E (Cypress) Coverage Collection
        if: steps.check_script.outputs.exists == 'true'
        run: |
          if [ -d ".nyc_output" ]; then
            COVERAGE_OUTPUT=$(npx nyc report --reporter=text-summary)

            echo "Coverage Output: $COVERAGE_OUTPUT"

            extract_raw() {
              echo "$COVERAGE_OUTPUT" | grep -oPm1 "^$1\s*:\s*\K([0-9.]+|Unknown)(?=%)"
            }

            # Pull the *raw* value (may be 'Unknown')
            RAW_STATEMENTS=$(extract_raw Statements)
            RAW_BRANCHES=$(extract_raw Branches)
            RAW_FUNCTIONS=$(extract_raw Functions)
            RAW_LINES=$(extract_raw Lines)

            # Check if all are Unknown first
            if [[ $RAW_STATEMENTS == Unknown && \
                $RAW_BRANCHES   == Unknown && \
                $RAW_FUNCTIONS  == Unknown && \
                $RAW_LINES      == Unknown ]]; then
              echo "::error::Coverage summary returned 'Unknown' for all metrics â€“ cannot proceed."
              exit 1
            fi

            # Warn about individual Unknown metrics
            UNKNOWN_COUNT=0
            [[ $RAW_STATEMENTS == Unknown ]] && { echo "::warning::Statements coverage is unavailable"; ((UNKNOWN_COUNT++)); }
            [[ $RAW_BRANCHES == Unknown ]] && { echo "::warning::Branches coverage is unavailable"; ((UNKNOWN_COUNT++)); }
            [[ $RAW_FUNCTIONS == Unknown ]] && { echo "::warning::Functions coverage is unavailable"; ((UNKNOWN_COUNT++)); }
            [[ $RAW_LINES == Unknown ]] && { echo "::warning::Lines coverage is unavailable"; ((UNKNOWN_COUNT++)); }

            # Convert to numbers (Unknown -> 0)
            to_num() { [[ $1 == Unknown ]] && echo 0 || echo "$1"; }

            STATEMENTS=$(to_num "$RAW_STATEMENTS")
            BRANCHES=$(to_num "$RAW_BRANCHES")
            FUNCTIONS=$(to_num "$RAW_FUNCTIONS")
            LINES=$(to_num "$RAW_LINES")

            echo "Line coverage:      ${LINES}%   (raw: ${RAW_LINES})"
            echo "Statement coverage: ${STATEMENTS}% (raw: ${RAW_STATEMENTS})"
            echo "Function coverage:  ${FUNCTIONS}% (raw: ${RAW_FUNCTIONS})"
            echo "Branch coverage:    ${BRANCHES}%  (raw: ${RAW_BRANCHES})"

            # Calculate average but note if partial
            if [[ $UNKNOWN_COUNT -gt 0 ]]; then
              AVERAGE_COVERAGE=$(echo "($LINES + $STATEMENTS + $FUNCTIONS + $BRANCHES) / 4" | bc -l)
              echo "Average coverage: $AVERAGE_COVERAGE% (Note: $UNKNOWN_COUNT metric(s) unavailable, counted as 0%)"
            else
              AVERAGE_COVERAGE=$(echo "($LINES + $STATEMENTS + $FUNCTIONS + $BRANCHES) / 4" | bc -l)
              echo "Average coverage: $AVERAGE_COVERAGE%"
            fi
          else
            echo ".nyc_output folder does not exist for coverage reporting"
            AVERAGE_COVERAGE=null
          fi

          echo "E2E_TEST_COVERAGE=$AVERAGE_COVERAGE" >> $GITHUB_ENV
          echo "E2E_TEST_COVERAGE_STATEMENTS=$STATEMENTS" >> $GITHUB_ENV
          echo "E2E_TEST_COVERAGE_BRANCHES=$BRANCHES" >> $GITHUB_ENV
          echo "E2E_TEST_COVERAGE_FUNCTIONS=$FUNCTIONS" >> $GITHUB_ENV
          echo "E2E_TEST_COVERAGE_LINES=$LINES" >> $GITHUB_ENV

      - name: Extract repo_name from package.json
        if: steps.check_script.outputs.exists == 'true'
        run: |
          repo_name=$(jq -r '.name' package.json)
          echo "repo_name=$repo_name" >> $GITHUB_ENV

      - name: Create audired-cypress-report.json
        if: steps.check_script.outputs.exists == 'true'
        run: |
          echo "{}" > audired-cypress-report.json
          report_json=$(cat audired-cypress-report.json)

          updated_report=$(echo "$report_json" | jq --arg repo_name "$repo_name" \
              --argjson e2e_test_coverage_statements "$E2E_TEST_COVERAGE_STATEMENTS" \
              --argjson e2e_test_coverage_branches "$E2E_TEST_COVERAGE_BRANCHES" \
              --argjson e2e_test_coverage_functions "$E2E_TEST_COVERAGE_FUNCTIONS" \
              --argjson e2e_test_coverage_lines "$E2E_TEST_COVERAGE_LINES" \
              --argjson e2e_test_coverage "$E2E_TEST_COVERAGE" \
              '. + {($repo_name): {"e2e_test_coverage_breakdown": {"e2e_test_coverage_statements": $e2e_test_coverage_statements, "e2e_test_coverage_branches": $e2e_test_coverage_branches, "e2e_test_coverage_functions": $e2e_test_coverage_functions, "e2e_test_coverage_lines": $e2e_test_coverage_lines}, "e2e_test_coverage": $e2e_test_coverage}}')

          echo "$updated_report" > audired-cypress-report.json

      - name: Upload report.json
        if: steps.check_script.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: audired-cypress-report
          path: audired-cypress-report.json
